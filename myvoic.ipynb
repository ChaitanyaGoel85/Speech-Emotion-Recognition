{"cells":[{"cell_type":"code","execution_count":1,"id":"da84d252","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-19T11:03:20.055100Z","iopub.status.busy":"2024-08-19T11:03:20.054302Z","iopub.status.idle":"2024-08-19T11:03:20.060214Z","shell.execute_reply":"2024-08-19T11:03:20.059045Z","shell.execute_reply.started":"2024-08-19T11:03:20.055069Z"},"papermill":{"duration":0.016992,"end_time":"2024-05-23T21:04:42.522413","exception":false,"start_time":"2024-05-23T21:04:42.505421","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# # This Python 3 environment comes with many helpful analytics libraries installed\n","# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# # For example, here's several helpful packages to load\n","\n","# import numpy as np # linear algebra\n","# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# # Input data files are available in the read-only \"../input/\" directory\n","# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# import os\n","\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"id":"945197a7","metadata":{"execution":{"iopub.execute_input":"2024-08-19T11:03:20.062335Z","iopub.status.busy":"2024-08-19T11:03:20.062017Z","iopub.status.idle":"2024-08-19T11:03:20.068289Z","shell.execute_reply":"2024-08-19T11:03:20.067545Z","shell.execute_reply.started":"2024-08-19T11:03:20.062306Z"},"trusted":true},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'myenv (Python 3.10.12)' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/home/voicerecog/myenv/bin/python -m pip install ipykernel -U --force-reinstall'"]}],"source":["# !pip install tqdm pandas wandb librosa numpy torchmetrics torch\n","# !pip3 install tqdm\n","# !pip install cuda\n","# !pip install build-essential dkms"]},{"cell_type":"code","execution_count":null,"id":"9a266d02","metadata":{"execution":{"iopub.execute_input":"2024-08-19T11:03:20.069963Z","iopub.status.busy":"2024-08-19T11:03:20.069373Z","iopub.status.idle":"2024-08-19T11:03:27.100721Z","shell.execute_reply":"2024-08-19T11:03:27.099729Z","shell.execute_reply.started":"2024-08-19T11:03:20.069934Z"},"papermill":{"duration":10.536445,"end_time":"2024-05-23T21:04:53.065941","exception":false,"start_time":"2024-05-23T21:04:42.529496","status":"completed"},"tags":[],"trusted":true},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'myenv (Python 3.10.12)' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/home/voicerecog/myenv/bin/python -m pip install ipykernel -U --force-reinstall'"]}],"source":["import warnings\n","from tqdm import tqdm\n","import os\n","from pathlib import Path\n","import math\n","import pandas as pd\n","\n","import wandb\n","\n","import librosa\n","\n","import numpy as np\n","\n","import torchmetrics\n","import torchmetrics.classification\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch.optim import lr_scheduler\n","from torch.optim.lr_scheduler import LambdaLR\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"id":"2113c209","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"a58338c4","metadata":{"papermill":{"duration":0.006334,"end_time":"2024-05-23T21:04:53.079031","exception":false,"start_time":"2024-05-23T21:04:53.072697","status":"completed"},"tags":[]},"source":["# Preparing the dataset\n","\n","This notebook will use the global audio datasets for the emotion recognition model. This will give us the relative success of the model compared to pre-existing models"]},{"cell_type":"code","execution_count":4,"id":"4b6867c2","metadata":{},"outputs":[],"source":["# ! pip install kaggle\n","# ! mkdir ~/.kaggle\n","# ! cp kaggle.json ~/.kaggle/\n","# ! chmod 600 ~/.kaggle/kaggle.json\n","# ! kaggle datasets download ejlok1/toronto-emotional-speech-set-tess\n","# ! unzip /content/toronto-emotional-speech-set-tess.zip\n","# !kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio\n","# !unzip ravdess-emotional-speech-audio.zip -d /home/voicerecog/ravdess-emotional-speech-audio/\n","\n","# !unzip ravdess-emotional-speech-audio.zip\n","# !unzip toronto-emotional-speech-set-tess.zip -d /home/voicerecog/toronto-emotional-speech-set-tess/\n","\n"]},{"cell_type":"markdown","id":"fd017247","metadata":{"papermill":{"duration":0.00621,"end_time":"2024-05-23T21:04:53.091649","exception":false,"start_time":"2024-05-23T21:04:53.085439","status":"completed"},"tags":[]},"source":["## Create csv file saving the location of the global datset.\n","\n","We are using the TESS, RAV, SAVEE and CREMA datasets"]},{"cell_type":"code","execution_count":5,"id":"b0ac2e7e","metadata":{"execution":{"iopub.execute_input":"2024-08-19T11:03:27.103034Z","iopub.status.busy":"2024-08-19T11:03:27.102592Z","iopub.status.idle":"2024-08-19T11:03:27.123755Z","shell.execute_reply":"2024-08-19T11:03:27.122901Z","shell.execute_reply.started":"2024-08-19T11:03:27.103010Z"},"papermill":{"duration":0.028784,"end_time":"2024-05-23T21:04:53.127217","exception":false,"start_time":"2024-05-23T21:04:53.098433","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["['Actor_22', 'Actor_11', 'Actor_12', 'Actor_20', 'Actor_23']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["#for dirname, _, filenames in os.walk('/kaggle/input'):\n","#    for filename in filenames:\n","#        print(os.path.join(dirname, filename))\n","\n","TESS = '/home/voicerecog/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/'\n","RAV = '/home/voicerecog/ravdess-emotional-speech-audio/audio_speech_actors_01-24/'\n","\n","# TESS = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n","# RAV = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n","# SAVEE = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"\n","# CREMA = \"/kaggle/input/cremad/AudioWAV/\"\n","\n","# Run one example \n","dir_list = os.listdir(RAV)\n","dir_list[0:5]"]},{"cell_type":"markdown","id":"143629a3","metadata":{"papermill":{"duration":0.006892,"end_time":"2024-05-23T21:04:53.141110","exception":false,"start_time":"2024-05-23T21:04:53.134218","status":"completed"},"tags":[]},"source":["RAVDESS editiong"]},{"cell_type":"code","execution_count":6,"id":"da8346a0","metadata":{"execution":{"iopub.execute_input":"2024-08-19T11:03:27.125631Z","iopub.status.busy":"2024-08-19T11:03:27.124947Z","iopub.status.idle":"2024-08-19T11:03:27.629765Z","shell.execute_reply":"2024-08-19T11:03:27.628744Z","shell.execute_reply.started":"2024-08-19T11:03:27.125599Z"},"papermill":{"duration":0.569335,"end_time":"2024-05-23T21:04:53.717565","exception":false,"start_time":"2024-05-23T21:04:53.148230","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["labels\n","male_neutral       144\n","female_neutral     144\n","male_sad            96\n","male_fear           96\n","male_angry          96\n","male_surprise       96\n","male_happy          96\n","male_disgust        96\n","female_angry        96\n","female_disgust      96\n","female_fear         96\n","female_surprise     96\n","female_happy        96\n","female_sad          96\n","Name: count, dtype: int64"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["dir_list = os.listdir(RAV)\n","dir_list.sort()\n","\n","emotion = []\n","gender = []\n","path = []\n","for i in dir_list:\n","    fname = os.listdir(RAV + i)\n","    for f in fname:\n","        part = f.split('.')[0].split('-')\n","        emotion.append(int(part[2]))\n","        temp = int(part[6])\n","        if temp%2 == 0:\n","            temp = \"female\"\n","        else:\n","            temp = \"male\"\n","        gender.append(temp)\n","        path.append(RAV + i + '/' + f)\n","\n","        \n","RAV_df = pd.DataFrame(emotion)\n","RAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\n","RAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\n","RAV_df.columns = ['gender','emotion']\n","RAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\n","RAV_df['source'] = 'RAVDESS'  \n","RAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n","RAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\n","RAV_df.labels.value_counts()"]},{"cell_type":"code","execution_count":7,"id":"63d564b1","metadata":{"execution":{"iopub.execute_input":"2024-08-19T11:03:27.631876Z","iopub.status.busy":"2024-08-19T11:03:27.631251Z","iopub.status.idle":"2024-08-19T11:03:28.445601Z","shell.execute_reply":"2024-08-19T11:03:28.444438Z","shell.execute_reply.started":"2024-08-19T11:03:27.631843Z"},"papermill":{"duration":0.991523,"end_time":"2024-05-23T21:04:54.716641","exception":false,"start_time":"2024-05-23T21:04:53.725118","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["labels\n","female_fear        400\n","female_surprise    400\n","female_sad         400\n","female_angry       400\n","female_disgust     400\n","female_happy       400\n","female_neutral     400\n","Name: count, dtype: int64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["dir_list = os.listdir(TESS)\n","dir_list.sort()\n","\n","path = []\n","emotion = []\n","\n","for i in dir_list:\n","    fname = os.listdir(TESS + i)\n","    for f in fname:\n","        if i == 'OAF_angry' or i == 'YAF_angry':\n","            emotion.append('female_angry')\n","        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n","            emotion.append('female_disgust')\n","        elif i == 'OAF_Fear' or i == 'YAF_fear':\n","            emotion.append('female_fear')\n","        elif i == 'OAF_happy' or i == 'YAF_happy':\n","            emotion.append('female_happy')\n","        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n","            emotion.append('female_neutral')                                \n","        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n","            emotion.append('female_surprise')               \n","        elif i == 'OAF_Sad' or i == 'YAF_sad':\n","            emotion.append('female_sad')\n","        else:\n","            emotion.append('Unknown')\n","        path.append(TESS + i + \"/\" + f)\n","\n","TESS_df = pd.DataFrame(emotion, columns = ['labels'])\n","TESS_df['source'] = 'TESS'\n","TESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n","TESS_df.labels.value_counts()"]},{"cell_type":"code","execution_count":8,"id":"6642033f","metadata":{"execution":{"iopub.execute_input":"2024-08-19T11:03:28.447522Z","iopub.status.busy":"2024-08-19T11:03:28.446844Z","iopub.status.idle":"2024-08-19T11:03:28.490067Z","shell.execute_reply":"2024-08-19T11:03:28.489260Z","shell.execute_reply.started":"2024-08-19T11:03:28.447497Z"},"papermill":{"duration":0.061873,"end_time":"2024-05-23T21:04:54.786162","exception":false,"start_time":"2024-05-23T21:04:54.724289","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["labels\n","female_neutral     544\n","female_surprise    496\n","female_fear        496\n","female_disgust     496\n","female_angry       496\n","female_happy       496\n","female_sad         496\n","male_neutral       144\n","male_happy          96\n","male_sad            96\n","male_surprise       96\n","male_angry          96\n","male_fear           96\n","male_disgust        96\n","Name: count, dtype: int64\n"]}],"source":["df = pd.concat([RAV_df, TESS_df], axis = 0)\n","print(df.labels.value_counts())\n","df.head()\n","df.to_csv(\"Data_path.csv\",index=False)"]},{"cell_type":"code","execution_count":9,"id":"c8575019","metadata":{"execution":{"iopub.execute_input":"2024-08-19T11:03:28.493045Z","iopub.status.busy":"2024-08-19T11:03:28.492793Z","iopub.status.idle":"2024-08-19T11:03:28.512034Z","shell.execute_reply":"2024-08-19T11:03:28.511394Z","shell.execute_reply.started":"2024-08-19T11:03:28.493024Z"},"papermill":{"duration":0.034733,"end_time":"2024-05-23T21:04:54.828632","exception":false,"start_time":"2024-05-23T21:04:54.793899","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["ref = pd.read_csv(\"/home/voicerecog/Data_path.csv\")"]},{"cell_type":"code","execution_count":10,"id":"f8faba96","metadata":{"execution":{"iopub.execute_input":"2024-08-19T11:03:28.513471Z","iopub.status.busy":"2024-08-19T11:03:28.513188Z","iopub.status.idle":"2024-08-19T11:03:28.662728Z","shell.execute_reply":"2024-08-19T11:03:28.661859Z","shell.execute_reply.started":"2024-08-19T11:03:28.513446Z"},"papermill":{"duration":0.028288,"end_time":"2024-05-23T21:04:54.865534","exception":false,"start_time":"2024-05-23T21:04:54.837246","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class AudioDataset(Dataset):\n","    def __init__(self, data, seq_len, d_model, augment=False):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.d_model = d_model\n","        self.data = data\n","        self.augment = augment\n","        self.num_augments = 1  # Default number of augmentations (original only)\n","\n","        if self.augment:\n","            self.num_augments = 4   # Add augment parameter\n","\n","        self.class_map = {\n","            \"female_neutral\": 0, \"female_surprise\": 1, \"female_disgust\": 2, \n","            \"female_fear\": 3, \"female_sad\": 4, \"female_happy\": 5, \"female_angry\": 6, \n","            \"male_neutral\": 7, \"male_sad\": 8, \"male_fear\": 9, \"male_happy\": 10, \n","            \"male_disgust\": 11, \"male_angry\": 12, \"male_surprise\": 13\n","        }\n","        self.label_map = {0: \"female_neutral\", 1: \"female_surprise\", 2: \"female_disgust\", \n","                          3: \"female_fear\", 4: \"female_sad\", 5: \"female_happy\", 6: \"female_angry\", \n","                          7: \"male_neutral\", 8: \"male_sad\", 9: \"male_fear\", 10: \"male_happy\",\n","                          11: \"male_disgust\", 12: \"male_angry\", 13: \"male_surprise\"}\n","\n","    @staticmethod\n","    def extract_audio_features(signal, sample_rate=44100):\n","        # MFCC\n","        mfccs = librosa.feature.mfcc(y=signal, n_mfcc=13, sr=sample_rate)\n","        delta_mfccs = librosa.feature.delta(mfccs)\n","        delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n","        # Root Mean Square Energy\n","#         root_mean_out = librosa.feature.rms(y=signal)\n","        mfccs_features = np.concatenate((mfccs, delta_mfccs, delta2_mfccs))\n","        return mfccs_features\n","\n","    def __len__(self):\n","        return len(self.data) * self.num_augments\n","\n","    \n","    def __getitem__(self, idx):\n","        # Determine which original sample and which augmentation to use\n","        original_idx = idx // self.num_augments\n","        augment_idx = idx % self.num_augments\n","\n","        file_path, emotion = self.data[original_idx]\n","        signal, sr = librosa.load(file_path)\n","\n","        # Create a list of signals (including augmentations)\n","        signals = [signal]  # Start with the original signal\n","\n","        if self.augment:\n","            noisy_signal = self.add_noise(signal)\n","            stretch_signal = self.stretch_process(signal)\n","            pitch_signal = self.pitch_process(signal, sr)\n","            signals.extend([noisy_signal, stretch_signal, pitch_signal])\n","\n","        # Select the signal based on the augmentation index\n","        sig = signals[augment_idx]\n","        inp = AudioDataset.extract_audio_features(sig, sr)\n","\n","        # Padding\n","        padding = self.seq_len - inp.shape[1]\n","        if padding < 0:\n","            raise ValueError(\"Audio is too long\")\n","        \n","        inp = torch.cat(\n","            [\n","                torch.tensor(inp, dtype=torch.float32),\n","                torch.zeros((self.d_model, padding), dtype=torch.float32)\n","            ], 1\n","        )\n","\n","        label = torch.zeros(14)\n","        label[self.class_map[emotion]] = 1\n","\n","        sample = {\n","            \"label\": label,\n","            \"file_path\": file_path,\n","            \"input\": inp.T,\n","            \"emotion\": emotion,\n","            \"class\": self.class_map[emotion]\n","        }\n","        \n","        return sample\n","\n","    \n","    @staticmethod\n","    def add_noise(signal, noise_level=0.005):\n","        noise = np.random.randn(len(signal))\n","        signal_noisy = signal + noise_level * noise\n","        return signal_noisy\n","\n","    @staticmethod\n","    def stretch_process(signal, rate=1.2):\n","        return librosa.effects.time_stretch(signal, rate=rate)\n","\n","    @staticmethod\n","    def pitch_process(signal, sr, n_steps=4):\n","        return librosa.effects.pitch_shift(signal, sr=sr, n_steps=n_steps)\n","\n","    \n","    \n","def causal_mask(size):\n","    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n","    return mask == 0\n","\n","\n","def get_label(vector):\n","    # class_map = {\"anger\" : 0, \"sadness\": 1, \"fear\": 2, \"happy\": 3, \"neutral\": 4, \"surprise\": 5, \"sarcastic\": 6, \"disgust\": 7}\n","    label_map = {0: \"anger\", 1: \"sadness\", 2: \"fear\", 3: \"happy\", 4: \"neutral\", 5: \"surprise\", 6: \"sarcastic\", 7: \"disgust\"}\n","\n","    return label_map[np.argmax(vector)]"]},{"cell_type":"code","execution_count":11,"id":"adbbdedb","metadata":{"execution":{"iopub.execute_input":"2024-08-19T11:03:28.664214Z","iopub.status.busy":"2024-08-19T11:03:28.663936Z","iopub.status.idle":"2024-08-19T11:03:28.715874Z","shell.execute_reply":"2024-08-19T11:03:28.715061Z","shell.execute_reply.started":"2024-08-19T11:03:28.664191Z"},"papermill":{"duration":0.074374,"end_time":"2024-05-23T21:04:54.947498","exception":false,"start_time":"2024-05-23T21:04:54.873124","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class LayerNormalization(nn.Module):\n","\n","    def __init__(self, features: int, eps:float=10**-6) -> None:\n","        super().__init__()\n","        self.eps = eps\n","        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n","        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n","\n","    def forward(self, x):\n","        # x: (batch, seq_len, hidden_size)\n","         # Keep the dimension for broadcasting\n","        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n","        # Keep the dimension for broadcasting\n","        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n","        # eps is to prevent dividing by zero or when std is very small\n","        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n","\n","class FeedForwardBlock(nn.Module):\n","\n","    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n","        super().__init__()\n","        self.dropout_1 = nn.Dropout(dropout)\n","        self.dropout_2 = nn.Dropout(dropout)\n","        self.dropout_3 = nn.Dropout(dropout)\n","        self.dropout_4 = nn.Dropout(dropout)\n","        \n","        # linear model\n","        self.linear_1 = nn.Linear(d_model, d_ff) \n","        self.linear_2 = nn.Linear(d_ff, d_ff)\n","        self.linear_3 = nn.Linear(d_ff, d_ff)\n","        self.linear_4 = nn.Linear(d_ff, d_ff)        \n","        self.linear_5 = nn.Linear(d_ff, d_model) \n","        \n","        # cnn model\n","        # self.conv_1 = nn.Conv2d(1, 8, kernel_size=(2, 2), padding=1, stride=1)\n","        # self.conv_2 = nn.Conv2d(8, 64, kernel_size=(2, 2), padding=1, stride=1)\n","        # self.conv_3 = nn.Conv2d(64, 128, kernel_size=(2, 2), padding=1, stride=1)\n","        # self.conv_4 = nn.Conv2d(128, 64, kernel_size=(2, 2), padding=1, stride=1)\n","        # self.conv_5 = nn.Conv2d(64, 8, kernel_size=(2, 2), padding=1, stride=1)\n","        # self.conv_6 = nn.Conv2d(8, 1, kernel_size=(2, 2), padding=1, stride=1)\n","\n","    def forward(self, x):\n","        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n","        x = self.linear_1(x)\n","        x = self.linear_2(self.dropout_1(torch.relu(x)))\n","        x = self.linear_3(self.dropout_2(torch.relu(x)))\n","        x = self.linear_4(self.dropout_3(torch.relu(x)))\n","        x = self.linear_5(self.dropout_4(torch.relu(x)))\n","        \n","        # convo forward \n","        # (batch, seq_len, d_model) --> (batch, 1, seq_len, d_model)\n","        # x = x.unsqeeze(1)\n","        \n","        return x\n","\n","    \n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n","        super().__init__()\n","        self.d_model = d_model\n","        self.seq_len = seq_len\n","        self.dropout = nn.Dropout(dropout)\n","        # Create a matrix of shape (seq_len, d_model)\n","        pe = torch.zeros(seq_len, d_model)\n","        # Create a vector of shape (seq_len)\n","        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n","        # Create a vector of shape (d_model)\n","        sin_div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n","        cos_div_term = torch.exp(torch.arange(1, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n","        # Apply sine to even indices\n","        pe[:, 0::2] = torch.sin(position * sin_div_term) # sin(position * (10000 ** (2i / d_model))\n","        # Apply cosine to odd indices\n","        pe[:, 1::2] = torch.cos(position * cos_div_term) # cos(position * (10000 ** (2i / d_model))\n","        # Add a batch dimension to the positional encoding\n","        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n","        # Register the positional encoding as a buffer\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n","        return self.dropout(x)\n","\n","class ResidualConnection(nn.Module):\n","    \n","        def __init__(self, features: int, dropout: float) -> None:\n","            super().__init__()\n","            self.dropout = nn.Dropout(dropout)\n","            self.norm = LayerNormalization(features)\n","    \n","        def forward(self, x, sublayer):\n","            return x + self.dropout(sublayer(self.norm(x)))\n","\n","class MultiHeadAttentionBlock(nn.Module):\n","\n","    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n","        super().__init__()\n","        self.d_model = d_model # Embedding vector size\n","        self.h = h # Number of heads\n","        # Make sure d_model is divisible by h\n","        assert d_model % h == 0, \"d_model is not divisible by h\"\n","\n","        self.d_k = d_model // h # Dimension of vector seen by each head\n","        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n","        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n","        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n","        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n","        self.dropout = nn.Dropout(dropout)\n","\n","    @staticmethod\n","    def attention(query, key, value, mask = None , dropout: nn.Dropout = None):\n","        d_k = query.shape[-1]\n","        # Just apply the formula from the paper\n","        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n","        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n","        if mask is not None:\n","            # Write a very low value (indicating -inf) to the positions where mask == 0\n","            attention_scores.masked_fill_(mask == 0, -1e9)\n","        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n","        if dropout is not None:\n","            attention_scores = dropout(attention_scores)\n","        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n","        # return attention scores which can be used for visualization\n","        return (attention_scores @ value), attention_scores\n","\n","    def forward(self, q, k, v, mask = None):\n","        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n","        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n","        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n","\n","        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n","        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n","        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n","        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n","\n","        # Calculate attention\n","        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n","        \n","        # Combine all the heads together\n","        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n","        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n","\n","        # Multiply by Wo\n","        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n","        return self.w_o(x)\n","\n","class EncoderBlock(nn.Module):\n","\n","    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n","        super().__init__()\n","        self.self_attention_block = self_attention_block\n","        self.feed_forward_block = feed_forward_block\n","        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n","\n","    def forward(self, x, src_mask = None):\n","        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n","        x = self.residual_connections[1](x, self.feed_forward_block)\n","        return x\n","    \n","    \n","class Encoder(nn.Module):\n","\n","    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n","        super().__init__()\n","        self.layers = layers\n","        self.norm = LayerNormalization(features)\n","\n","    def forward(self, x, mask = None):\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)\n","\n","\n","class ProjectionLayer(nn.Module):\n","\n","    def __init__(self, seq_len: int, d_model: int, d_ff: int, num_of_labels: int, dropout: float) -> None:\n","        super().__init__()\n","        self.dropout_1 = nn.Dropout(dropout)\n","        self.dropout_2 = nn.Dropout(dropout)\n","        self.dropout_3 = nn.Dropout(dropout)\n","        self.dropout_4 = nn.Dropout(dropout)\n","        self.dropout_5 = nn.Dropout(dropout)\n","        self.dropout_6 = nn.Dropout(dropout)\n","        \n","        self.proj = nn.Linear(d_ff, num_of_labels)\n","\n","#         self.proj = nn.Linear(512, num_of_labels)\n","        # dense network\n","#         self.linear_1 = nn.Linear(d_model, d_ff)\n","#         self.linear_2 = nn.Linear(d_ff, d_ff)\n","#         self.linear_3 = nn.Linear(d_ff, d_model)\n","#         self.linear_4 = nn.Linear(d_model*seq_len, d_ff)\n","#         self.linear_5 = nn.Linear(d_ff, d_ff)\n","#         self.linear_6 = nn.Linear(d_ff, d_ff)\n","        \n","        \n","        # cnn model\n","        # self.conv_1 = nn.Conv2d(1, 8, padding_mode='replicate')\n","        # self.conv_2 = nn.Conv2d(8, 64, padding_mode='replicate')\n","        # self.conv_3 = nn.Conv2d(64, 8, padding_mode='replicate')\n","        # self.conv_4 = nn.Conv2d(8, 1, padding_mode='replicate')\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n","        self.batchnorm_1 = nn.BatchNorm2d(8)\n","        self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","        self.conv2 = nn.Conv2d(in_channels=8, out_channels=64, kernel_size=3, stride=1, padding=1)\n","        self.batchnorm_2 = nn.BatchNorm2d(64)\n","        self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n","        self.batchnorm_3 = nn.BatchNorm2d(128)\n","        self.pool_3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","        self.conv4 = nn.Conv2d(in_channels=128, out_channels=d_ff, kernel_size=(75, 4), stride=1, padding=0)\n","        self.batchnorm_4 = nn.BatchNorm2d(d_ff)\n","\n","\n","    def forward(self, x) -> None:\n","        # (batch, seq_len, d_model) --> (batch, num_of_labels)\n","#         x = self.linear_1(x)\n","#         x = self.linear_2(self.dropout_1(torch.relu(x)))\n","#         x = self.linear_3(self.dropout_2(torch.relu(x)))\n","#         x = torch.flatten(x,1)\n","#         x = self.linear_4(self.dropout_3(torch.relu(x)))\n","#         x = self.linear_5(self.dropout_4(torch.relu(x)))\n","#         x = self.linear_6(self.dropout_5(torch.relu(x)))\n","#         x = self.proj(self.dropout_6(torch.relu(x)))\n","        \n","        # convo forward \n","        # (batch, seq_len, d_model) --> (batch, 1, seq_len, d_model)\n","        x = torch.unsqueeze(x, 1)\n","        x = self.dropout_1(self.pool_1(torch.relu(self.batchnorm_1(self.conv1(x)))))\n","        x = self.dropout_2(self.pool_2(torch.relu(self.batchnorm_2(self.conv2(x)))))\n","        x = self.dropout_3(self.pool_3(torch.relu(self.batchnorm_3(self.conv3(x)))))\n","        x = self.dropout_4(torch.relu(self.batchnorm_4(self.conv4(x))))\n","        \n","        x = torch.flatten(x, 1)\n","        x = self.proj(x)\n","        \n","        return x\n","\n","\n","\n","    \n","class Transformer(nn.Module):\n","\n","    def __init__(self, encoder: Encoder, inp_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n","        super().__init__()\n","        self.encoder = encoder\n","        self.inp_pos = inp_pos\n","        self.projection_layer = projection_layer\n","\n","    def encode(self, inp, inp_mask = None):\n","        # (batch, seq_len, d_model)\n","        inp = self.inp_pos(inp)\n","        return self.encoder(inp, inp_mask)\n","    \n","    def project(self, x):\n","        # (batch, num_of_label)\n","        return self.projection_layer(x)\n","    \n","\n","\n","def build_transformer(seq_len: int, num_of_labels: int, d_model: int=39, N: int=5, h: int=3, dropout: float=0.1, d_ff: int=256) -> Transformer:\n","    # Create the embedding layers\n","\n","    # Create the positional encoding layers\n","    inp_pos = PositionalEncoding(d_model, seq_len, dropout)\n","    \n","    # Create the encoder blocks\n","    encoder_blocks = []\n","    for _ in range(N):\n","        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n","        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n","        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n","        encoder_blocks.append(encoder_block)\n","\n","    \n","    # Create the encoder and decoder\n","    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n","    \n","    # Create the projection layer\n","    projection_layer = ProjectionLayer(seq_len, d_model, d_ff, num_of_labels, dropout)\n","    \n","    # Create the transformer\n","    transformer = Transformer(encoder, inp_pos, projection_layer)\n","    transformer.to(device)\n","    \n","    # Initialize the parameters\n","    for p in transformer.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)\n","    \n","    return transformer\n","\n","\n","class SquareRootScheduler:\n","    def __init__(self, lr=0.1):\n","        self.lr = lr\n","\n","    def __call__(self, num_update):\n","        return self.lr * pow(num_update + 1.0, -0.5)"]},{"cell_type":"code","execution_count":12,"id":"0d11c547","metadata":{"execution":{"iopub.execute_input":"2024-08-19T11:03:28.717079Z","iopub.status.busy":"2024-08-19T11:03:28.716843Z","iopub.status.idle":"2024-08-19T11:03:28.729240Z","shell.execute_reply":"2024-08-19T11:03:28.728460Z","shell.execute_reply.started":"2024-08-19T11:03:28.717059Z"},"papermill":{"duration":0.020451,"end_time":"2024-05-23T21:04:54.976100","exception":false,"start_time":"2024-05-23T21:04:54.955649","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def get_config():\n","    return {\n","        \"dataset_root_location\": \"/kaggle/working/\",\n","        \"df_location\": \"Data_path.csv\",\n","        \"num_of_labels\": 14,\n","        \"batch_size\": 16,\n","        \"num_epochs\": 50,\n","        \"lr\": 0.1,\n","        \"seq_len\": 600,\n","        \"d_model\": 39,\n","        \"datasource\": \"Global_dataset_ravdess_tess\",\n","        \"model_folder\": \"weights\",\n","        \"model_basename\": \"tmodel_\",\n","        \"preload\": \"latest\",\n","        \"experiment_name\": \"runs/tmodel\"\n","    }\n","\n","def get_weights_file_path(config, epoch: str):\n","    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n","    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n","    return str(Path('.') / model_folder / model_filename)\n","\n","# Find the latest weights file in the weights folder\n","def latest_weights_file_path(config):\n","    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n","    model_filename = f\"{config['model_basename']}*\"\n","    weights_files = list(Path(model_folder).glob(model_filename))\n","    if len(weights_files) == 0:\n","        return None\n","    weights_files.sort()\n","    return str(weights_files[-1])\n"]},{"cell_type":"code","execution_count":13,"id":"f12c55e3","metadata":{},"outputs":[],"source":["# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# print(\"Using device:\", device)\n","# !pip install nvidia-cuda-toolkit\n","# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","# import torch\n","# print(\"CUDA Available:\", torch.cuda.is_available())\n","# print(\"Number of GPUs:\", torch.cuda.device_count())\n","# print(\"Current GPU:\", torch.cuda.current_device())\n","# print(\"Device Name:\", torch.cuda.get_device_name(0))\n","# # !pip install nvidia-pyindex\n"]},{"cell_type":"code","execution_count":14,"id":"2e17f41c","metadata":{"execution":{"iopub.execute_input":"2024-08-19T11:03:28.730542Z","iopub.status.busy":"2024-08-19T11:03:28.730227Z","iopub.status.idle":"2024-08-19T11:03:28.758284Z","shell.execute_reply":"2024-08-19T11:03:28.757538Z","shell.execute_reply.started":"2024-08-19T11:03:28.730520Z"},"papermill":{"duration":0.043588,"end_time":"2024-05-23T21:04:55.027506","exception":false,"start_time":"2024-05-23T21:04:54.983918","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def run_validation(model, validation_ds, config, device, global_step):\n","    model.eval()\n","    \n","    #     # get the console window width\n","    #     with os.popen('stty size', 'r') as console:\n","    #         _, console_width = console.read().split()\n","    #         console_width = int(console_width)\n","    # except:\n","    #     # If we can't get the console width, use 80 as default\n","    #     console_width = 80\n","\n","    with torch.no_grad():\n","        for batch in validation_ds:\n","            count += 1\n","            inp = batch[\"input\"].to(device) # (b, d_model, seq_len)\n","            inp_mask = None\n","\n","            # check that the batch size is 1\n","            assert inp.size(\n","                0) == 1, \"Batch size must be 1 for validation\"\n","\n","            out = model.encode(inp)\n","            out = model.project(out).squeeze(0)\n","            # pred_class = (np.zeros_like(out) == 0)[np.argmax(out)] = 1\n","\n","            # storing results\n","            pred_classes.append(torch.argmax(out).item())\n","            true_classes.append(torch.argmax(batch[\"label\"].squeeze(0)).item())\n","        true_classes = torch.tensor(true_classes)\n","        pred_classes = torch.tensor(pred_classes)\n","        # Print the source, target and model output\n","        # print_msg('-'*console_width)\n","        # print_msg(f\"{f'TARGET: ':>12}{label}\")3\n","        # print_msg(f\"{f'PREDICTED: ':>12}{pred_out}\")\n","        # print_msg('-'*console_width)\n","    \n","    \n","    # Evaluate the character error rate\n","    # Compute the char error rate \n","    # metric = torchmetrics.classification.MulticlassConfusionMatrix(num_classes=config[\"num_of_labels\"])\n","    # cer = metric(pred_classes, true_classes)\n","    # wandb.log({'validation/cer': cer, 'global_step': global_step})\n","\n","    # Compute confusion matrix\n","    confusion_matrix = torchmetrics.ConfusionMatrix(num_classes=config[\"num_of_labels\"], task=\"multiclass\")\n","    confusion_matrix(pred_classes, true_classes)\n","\n","    # Compute F1-score\n","    f1_score = torchmetrics.classification.MulticlassF1Score(num_classes=config[\"num_of_labels\"])\n","    f1_score(pred_classes, true_classes)\n","\n","    # Compute accuracy\n","    accuracy = torchmetrics.classification.MulticlassAccuracy(num_classes=config[\"num_of_labels\"])\n","    accuracy(pred_classes, true_classes) \n","\n","    # Compute recall\n","    recall = torchmetrics.classification.MulticlassRecall(num_classes=config[\"num_of_labels\"])\n","    recall(pred_classes, true_classes)   \n","\n","    # Compute precision\n","    precision = torchmetrics.classification.MulticlassPrecision(num_classes=config[\"num_of_labels\"])\n","    precision(pred_classes, true_classes)\n","\n","    # Log metrics to WandB\n","    wandb.log({\"validation/confusion_matrix\": confusion_matrix.compute(),\n","               \"validation/accuracy\": accuracy.compute(), \n","               \"validation/recall\": recall.compute(), \n","               \"validation/precision\": precision.compute(), \n","               \"validation/f1_score\": f1_score.compute(),\n","               'global_step': global_step})\n","\n","\n","def get_audio_location_list(df_location):\n","    data = []\n","    ref = pd.read_csv(df_location)\n","    for index, row in ref.iterrows():\n","        data.append([row['path'], row['labels']])\n","\n","    return data\n","\n","def get_ds(config):\n","    # It only has the train split, so we divide it overselves\n","    ds_raw = get_audio_location_list(config['df_location'])\n","    \n","    # Keep 90% for training, 10% for validation\n","    train_ds_raw_size = int(0.9 * len(ds_raw))\n","    val_ds_raw_size = len(ds_raw) - train_ds_raw_size\n","    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_raw_size, val_ds_raw_size])\n","\n","#     train_ds = AudioDataset(train_ds_raw, config[\"seq_len\"], config[\"d_model\"])\n","#     val_ds = AudioDataset(val_ds_raw, config[\"seq_len\"], config[\"d_model\"])\n","    train_ds = AudioDataset(train_ds_raw, config[\"seq_len\"], config[\"d_model\"],augment=True)\n","    val_ds = AudioDataset(val_ds_raw, config[\"seq_len\"], config[\"d_model\"], augment=False)\n","    \n","#     print(train_ds_raw_size)\n","#     print(len(train_ds))\n","\n","#     train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n","#     val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n","    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, num_workers=3, pin_memory=True)\n","    \n","    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True, num_workers=3, pin_memory=True)\n","\n","    return train_dataloader, val_dataloader\n","\n","def get_model(config):\n","    model = build_transformer( config[\"seq_len\"], config['num_of_labels'], d_model=config['d_model'])\n","    return model\n","\n","def train_model(config):\n","    # Define the device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"Using device:\", device)\n","\n","    # Make sure the weights folder exists\n","    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n","\n","    train_dataloader, val_dataloader = get_ds(config)\n","    model = get_model(config).to(device)\n","\n","    optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'])\n","    \n","    scheduler = SquareRootScheduler(lr=config['lr'])\n","\n","    # If the user specified a model to preload before training, load it\n","    initial_epoch = 0\n","    global_step = 0\n","    if config['preload']:\n","        model_filename = latest_weights_file_path(config)\n","        print(f'Preloading model {model_filename}')\n","        state = torch.load(model_filename)\n","        model.load_state_dict(state['model_state_dict'])\n","        initial_epoch = state['epoch'] + 1\n","        optimizer.load_state_dict(state['optimizer_state_dict'])\n","        global_step = state['global_step']\n","        del state\n","\n","    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n","\n","    # define our custom x axis metric\n","    wandb.define_metric(\"global_step\")\n","    # define which metrics will be plotted against it\n","    wandb.define_metric(\"validation/*\", step_metric=\"global_step\")\n","    wandb.define_metric(\"train/*\", step_metric=\"global_step\")\n","\n","    for epoch in range(initial_epoch, initial_epoch+config['num_epochs']):\n","        torch.cuda.empty_cache()\n","#         model.train()\n","        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n","        for batch in batch_iterator:\n","            \n","            # moved here\n","            model.train()\n","            optimizer.zero_grad()\n","\n","            input = batch['input'].to(device) # (b, seq_len, d_model)\n","\n","            # Run the tensors through the encoder, decoder and the projection layer\n","            encoder_output = model.encode(input) # (B, seq_len, d_model)\n","            proj_output = model.project(encoder_output) # (B, num_of_labels)\n","\n","            # Compare the output with the label\n","            label = batch['label'].to(device) # (B, num_of_labels)\n","\n","            # Compute the loss using a simple cross entropy\n","            loss = loss_fn(proj_output, label)\n","            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n","\n","            # Log the loss\n","            wandb.log({'train/loss': loss.item(), 'global_step': global_step})\n","\n","            # Backpropagate the loss\n","            loss.backward()\n","\n","            # Update the weights\n","            optimizer.step()\n","#             optimizer.zero_grad(set_to_none=True)\n","\n","        global_step += 1\n","        \n","        # updating scheduler\n","        if scheduler:\n","            if scheduler.__module__ == lr_scheduler.__name__:\n","                # Using PyTorch In-Built scheduler\n","                scheduler.step()\n","            else:\n","                # Using custom defined scheduler\n","                for param_group in optimizer.param_groups:\n","                    param_group['lr'] = scheduler(epoch)\n","\n","        # Run validation at the end of every epochnbv\n","        run_validation(model, val_dataloader, config, device, global_step)\n","\n","        # Save the model at the end of every epoch\n","        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'global_step': global_step\n","        }, model_filename)"]},{"cell_type":"code","execution_count":15,"id":"9299b3f8","metadata":{"execution":{"iopub.execute_input":"2024-08-19T11:03:28.759778Z","iopub.status.busy":"2024-08-19T11:03:28.759520Z","iopub.status.idle":"2024-08-19T11:03:28.771913Z","shell.execute_reply":"2024-08-19T11:03:28.771237Z","shell.execute_reply.started":"2024-08-19T11:03:28.759757Z"},"trusted":true},"outputs":[],"source":["# #I have added this to check input data\n","# config = get_config()\n","# def get_audio_location_list(df_location):\n","#     data = []\n","#     ref = pd.read_csv(df_location)\n","#     for index, row in ref.iterrows():\n","#         data.append([row['path'], row['labels']])\n","\n","#     return data\n","\n","# def get_ds(config):\n","#     # It only has the train split, so we divide it overselves\n","#     ds_raw = get_audio_location_list(config['df_location'])\n","    \n","#     # Keep 90% for training, 10% for validation\n","#     train_ds_raw_size = int(0.9 * len(ds_raw))\n","#     val_ds_raw_size = len(ds_raw) - train_ds_raw_size\n","#     train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_raw_size, val_ds_raw_size])\n","\n","# #     train_ds = AudioDataset(train_ds_raw, config[\"seq_len\"], config[\"d_model\"])\n","# #     val_ds = AudioDataset(val_ds_raw, config[\"seq_len\"], config[\"d_model\"])\n","#     train_ds = AudioDataset(train_ds_raw, config[\"seq_len\"], config[\"d_model\"],augment=True)\n","#     val_ds = AudioDataset(val_ds_raw, config[\"seq_len\"], config[\"d_model\"], augment=False)\n","    \n","#     print(train_ds_raw_size)\n","#     print(len(train_ds))\n","\n","# #     train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n","# #     val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n","#     train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, num_workers=4, pin_memory=True)\n","    \n","#     val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n","\n","#     return train_dataloader, val_dataloader\n","\n","\n","\n","# # Make sure the weights folder exists\n","# Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n","# train_dataloader, val_dataloader = get_ds(config)\n","# initial_epoch = 0\n","# global_step = 0\n","# for epoch in range(initial_epoch, initial_epoch+config['num_epochs']):\n","#         torch.cuda.empty_cache()\n","#         batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n","#         for batch in batch_iterator:\n","#             input = batch['input'] # (b, seq_len, d_model)\n","# #             print(input.shape) #torch.Size([16, 600, 39])"]},{"cell_type":"code","execution_count":null,"id":"fb9d3255","metadata":{"execution":{"iopub.execute_input":"2024-08-19T11:03:28.773161Z","iopub.status.busy":"2024-08-19T11:03:28.772885Z","iopub.status.idle":"2024-08-19T11:03:28.783859Z","shell.execute_reply":"2024-08-19T11:03:28.783043Z","shell.execute_reply.started":"2024-08-19T11:03:28.773131Z"},"papermill":{"duration":0.016078,"end_time":"2024-05-23T21:04:55.051212","exception":false,"start_time":"2024-05-23T21:04:55.035134","status":"completed"},"tags":[],"trusted":true},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'myenv (Python 3.10.12)' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/home/voicerecog/myenv/bin/python -m pip install ipykernel -U --force-reinstall'"]}],"source":["if not os.path.exists(\"Global_dataset_ravdess_tess_weights\"):\n","    os.mkdir(\"Global_dataset_ravdess_tess_weights\")\n","\n","os.makedirs(\"Global_dataset_ravdess_tess_weights\", exist_ok=True)"]},{"cell_type":"code","execution_count":17,"id":"2d28aad2","metadata":{"execution":{"iopub.execute_input":"2024-08-19T11:03:28.785169Z","iopub.status.busy":"2024-08-19T11:03:28.784861Z"},"papermill":{"duration":15710.318883,"end_time":"2024-05-24T01:26:45.378404","exception":false,"start_time":"2024-05-23T21:04:55.059521","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.2) or chardet (4.0.0) doesn't match a supported version!\n","  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpranjalverma224\u001b[0m (\u001b[33mpranjalverma224-iit-delhi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/voicerecog/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.17.7"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/voicerecog/wandb/run-20240826_134123-d8kymyqq</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/pranjalverma224-iit-delhi/pytorch-transformer/runs/d8kymyqq' target=\"_blank\">treasured-snow-94</a></strong> to <a href='https://wandb.ai/pranjalverma224-iit-delhi/pytorch-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/pranjalverma224-iit-delhi/pytorch-transformer' target=\"_blank\">https://wandb.ai/pranjalverma224-iit-delhi/pytorch-transformer</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/pranjalverma224-iit-delhi/pytorch-transformer/runs/d8kymyqq' target=\"_blank\">https://wandb.ai/pranjalverma224-iit-delhi/pytorch-transformer/runs/d8kymyqq</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Using device: cpu\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 00:   2%|         | 23/954 [00:43<29:31,  1.90s/it, loss=2.315] \n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 20\u001b[0m\n\u001b[1;32m      9\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlogin(key\u001b[38;5;241m=\u001b[39mWANDB_API_KEY)\n\u001b[1;32m     11\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# set the wandb project where this run will be logged\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch-transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[14], line 159\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (b, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Run the tensors through the encoder, decoder and the projection layer\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m    160\u001b[0m proj_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mproject(encoder_output) \u001b[38;5;66;03m# (B, num_of_labels)\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Compare the output with the label\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 253\u001b[0m, in \u001b[0;36mTransformer.encode\u001b[0;34m(self, inp, inp_mask)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp, inp_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# (batch, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minp_pos(inp)\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp_mask\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 170\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 170\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 156\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, x, src_mask)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, src_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 156\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_connections\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m1\u001b[39m](x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_block)\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 92\u001b[0m, in \u001b[0;36mResidualConnection.forward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, sublayer):\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n","Cell \u001b[0;32mIn[11], line 156\u001b[0m, in \u001b[0;36mEncoderBlock.forward.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, src_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 156\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m0\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    157\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m1\u001b[39m](x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_block)\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 137\u001b[0m, in \u001b[0;36mMultiHeadAttentionBlock.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m    134\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mview(value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Calculate attention\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mMultiHeadAttentionBlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Combine all the heads together\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["warnings.filterwarnings(\"ignore\")\n","config = get_config()\n","config['num_epochs'] = 100\n","config['preload'] = None\n","\n","# WANDB_API_KEY = \"206bdc05624d9a09ecd40f918c8fcf562794eeba\"\n","WANDB_API_KEY = \"5917e81ca177aa743d512c104ea62b82dc6da9f5\"\n","# WANDB_API_KEY = \"5917e81ca177aa743d512c104ea62b82dc6da9f5\"\n","wandb.login(key=WANDB_API_KEY)\n","\n","wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"pytorch-transformer\",\n","    \n","    # track hyperparameters and run metadata\n","    config=config\n",")\n","\n","\n","train_model(config)"]},{"cell_type":"code","execution_count":null,"id":"90e4599a","metadata":{"papermill":{"duration":4.41924,"end_time":"2024-05-24T01:26:54.363684","exception":false,"start_time":"2024-05-24T01:26:49.944444","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# wandb login --relogin  "]},{"cell_type":"code","execution_count":null,"id":"c6556492","metadata":{"papermill":{"duration":4.486513,"end_time":"2024-05-24T01:27:03.371812","exception":false,"start_time":"2024-05-24T01:26:58.885299","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# !pip install wandb\n","# !wandb login\n","WANDB_API_KEY = \"5917e81ca177aa743d512c104ea62b82dc6da9f5\"\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":107620,"sourceId":256618,"sourceType":"datasetVersion"},{"datasetId":251883,"sourceId":529602,"sourceType":"datasetVersion"},{"datasetId":316368,"sourceId":639622,"sourceType":"datasetVersion"},{"datasetId":4660900,"sourceId":7929891,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":15751.335657,"end_time":"2024-05-24T01:27:10.693636","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-23T21:04:39.357979","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}
